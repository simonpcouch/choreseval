#' Evaluation results
#'
#' @description
#' The `chores` data contains "confirmed" evaluation results generated by
#' running the following:
#'
#' ```
#' tsk <- chores_task()
#' tsk$eval(
#'   solver_chat = ellmer::chat_*(model = "some-model")
#' )
#' ```
#'
#' See [chores_task()] for more on how the evaluation task works. Notably:
#'
#' * The solver carries out `r nrow(chores_dataset)` refactorings using
#'   the [cli chore helper][chores::`doc-helper-cli`], each repeated 3 times.
#' * Each refactoring is then graded according to a rubric using Claude 4
#'   Sonnet. The grading results in a score between 0 and 1 and incorporates
#'   measures of code quality as well as execution time.
#' * Grading costs something like $2.50; the cost of solving depends on the
#'   model pricing.
#'
#' @section Columns:
#' * `provider`:
#' * `model`:
#' * `score`:
#' * `metadata`:
"chores"

chores_eval <- function(solver_chat, name = task_name(solver_chat)) {
  tsk <- chores_task()
  tsk$eval(solver_chat = solver_chat)
  saveRDS(
    tsk,
    file = paste0("data-raw/chores/tasks/", name, ".rds")
  )
  source("data-raw/chores.R")
}

task_name <- function(chat) {
  provider <- chat$get_provider()
  paste0(provider@name, "-", provider@model)
}
