[{"path":"https://simonpcouch.github.io/choreseval/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 Posit Software, PBC Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://simonpcouch.github.io/choreseval/articles/motivation.html","id":"the-eval","dir":"Articles","previous_headings":"","what":"The eval","title":"Motivation","text":"chores eval measures well model perform model powering chores. good fit chores, model needs 1) write code decently well 2) good instruction-following. eval implemented vitals, new R package large language model evaluation ’ve working . vitals, evals composed (least) three pieces: dataset, solver, scorer. dataset , minimally data frame labelled samples input target. case, input composed pieces code “wild” need refactored. example, chores “cli” helper refactor erroring code purrr use cli package: successful conversion cli transition cli::cli_abort(), apply inline markup .x note ’s argument (code), transition argument value call. = FALSE cli analogue call = NULL. ’s target grading guidance input: next core piece solver, function takes input , via LLM-based tool, processes input hopefully output something like target. case eval, solver applies cli chore system prompt passes input user turn, returning whatever model returns. model powering solver important “independent variable” . final element scorer. eval uses model grading, cli chore system prompt, input, target, solver output concatenated one string. , passed another LLM, point model asked make use context score solver output according rubric. rubric includes 14 grading criteria can “Yes”, “”, NA, NA indicates grading criterion isn’t applicable given refactor. score question calculate n(\"Yes\") / (n(\"Yes\") + n(\"\")). two catches : solver doesn’t output valid R code given input, response given score 0. Solvers given 2-second “grace period” response finish streaming. , additional second results point subtracted numerator (minimum score 0). end eval, scores averaged multiplied 100 generate percentage.","code":"stop(\"Cannot coerce .x to a vector\", call. = FALSE) ``` cli::cli_abort(   \"Can't coerce {.arg .x} to a vector.\" ) ```  Can also have `call = NULL`, i.e.:  ``` cli::cli_abort(   \"Can't coerce {.arg .x} to a vector.\",    call = NULL ) ```  The backticks are for example only and should not be included in the desired output."},{"path":"https://simonpcouch.github.io/choreseval/articles/motivation.html","id":"initial-benchmarks","dir":"Articles","previous_headings":"","what":"Initial benchmarks","title":"Motivation","text":"usual advice writing LLM evaluations , eval first released, state---art performance really low. Like, single digits low. use case eval bit different. case, already model works really well, don’t need better. Claude 4 Sonnet already performs almost perfectly, many models. didn’t super well, bug eval rather shortcoming model—problems easily solvable strong LLMs. thing ’m actually interested small /cheap can model still performing well? Ideally, ’d like 8-billion parameter model, even 4- 2- 1-billion parameter model, can return correctly refactored code within couple seconds. case, users laptop 8GB 4GB RAM feasibly well. consistency, ’ll running evals laptop. ’s 4-year old—relatively maxed-time—16” Macbook Pro. grand scheme “computers people write R code ,” pretty fancy. , ’s sort “weight class” dynamic . Expensive models require specialized hardware run shouldn’t compared something can served locally iPhone; recommend thinking models belonging one classes, opening different modes using communicating chores. Strong LLMs: models couple dollars per million input tokens. Claude 3.7 Sonnet $3.00, GPT-4o $3.75 -4.1 $2.00, Gemini 2.5 Pro $1.25. models really well eval; ’ve ran developed eval don’t think ’ll new models come . Budget LLMs: models $0.50 $0.10 per million inputs tokens. example, GPT 4.1-mini $0.40 -nano $0.10. Gemini 2.0 Flash $0.35. example GPT 4.1-nano, thirty times cheaper Claude 3.7 Sonnet. ’s 2,000 refactors per dollar. model capable helper, confidently tell user “Put $1.00 API key never reload .” Local LLMs: Anything can run Macbook. system 32GB RAM, 32-billion parameter model probably run slowly speed penalty overtake strong performance. guess , moment, couple models available today >50% eval. SmoLLMs: Maximum 4-billion parameters. ultimately ’m really interested .","code":""},{"path":"https://simonpcouch.github.io/choreseval/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Simon Couch. Author, maintainer. Posit Software, PBC. Copyright holder, funder.","code":""},{"path":"https://simonpcouch.github.io/choreseval/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Couch S (2025). choreseval: Chores Evaluation. R package version 0.0.0.9000, https://github.com/simonpcouch/choreseval.","code":"@Manual{,   title = {choreseval: The Chores Evaluation},   author = {Simon Couch},   year = {2025},   note = {R package version 0.0.0.9000},   url = {https://github.com/simonpcouch/choreseval}, }"},{"path":"https://simonpcouch.github.io/choreseval/index.html","id":"the-chores-eval","dir":"","previous_headings":"","what":"The Chores Evaluation","title":"The Chores Evaluation","text":"chores package connects extensible library LLM assistants IDE aimed helping tedious hard--automate tasks. capable LLMs today, like Anthropic’s Claude 4 Sonnet OpenAI’s GPT 4.1, carrying sorts tasks chores supports easy peasy. However, models cost money (require entrusting IP -profit company), ’s bit hoot ask R users put credit card information use R package. made wonder: possible use chores locally running LLM? chores eval evaluates well large language model perform model powering chores goal helping identify model chores users can run modest laptop. chores eval implemented vitals, LLM eval framework R.","code":""},{"path":"https://simonpcouch.github.io/choreseval/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"The Chores Evaluation","text":"choreseval implemented R package ease installation: Load :","code":"pak::pak(\"simonpcouch/choreseval\") library(choreseval)"},{"path":"https://simonpcouch.github.io/choreseval/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"The Chores Evaluation","text":"reader eval, ’re mostly likely interested chores, dataset compiled evaluation results:  chores_task() function defines task package’s built-dataset, solver, scorer: Run $eval() solver_chat choice measure well model eval:","code":"library(tibble)  chores #> # A tibble: 8 × 6 #>   name                          provider      model         score price metadata #>   <chr>                         <chr>         <chr>         <dbl> <chr> <list>   #> 1 claude_opus_4                 Anthropic     claude-opus-… 0.917 $7.52 <tibble> #> 2 claude_sonnet_4               Anthropic     claude-sonne… 0.939 $1.51 <tibble> #> 3 gemini_2_5_flash_non_thinking Google/Gemini gemini-2.5-f… 0.874 $0.11 <tibble> #> 4 gemini_2_5_flash_thinking     Google/Gemini gemini-2.5-f… 0.897 $0.10 <tibble> #> 5 gemini_2_5_pro                Google/Gemini gemini-2.5-p… 0.918 $0.44 <tibble> #> 6 gpt_4_1_mini                  OpenAI        gpt-4.1-mini  0.855 $0.06 <tibble> #> 7 gpt_4_1_nano                  OpenAI        gpt-4.1-nano  0.774 $0.01 <tibble> #> 8 gpt_4_1                       OpenAI        gpt-4.1       0.904 $0.29 <tibble> library(tidyverse) library(ggrepel)  chores |>   mutate(price = as.numeric(gsub(\"$\", \"\", price, fixed = TRUE))) |>   ggplot(aes(x = price, y = score, color = provider, label = model)) +   geom_point() +   geom_label_repel() +   scale_x_log10(labels = scales::dollar_format()) +   theme_minimal() +   labs(x = \"Price (USD, per 100 refactorings)\", y = \"Score\") tsk <- chores_task()  tsk #> An evaluation task The-chores-eval. tsk$eval(   solver_chat = ellmer::chat_anthropic(model = \"claude-3-7-sonnet-latest\") )"},{"path":"https://simonpcouch.github.io/choreseval/reference/chores.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluation results — chores","title":"Evaluation results — chores","text":"chores data contains \"confirmed\" evaluation results generated running following:   See chores_task() evaluation task works. Notably: solver carries 34 refactorings using cli chore helper, repeated 3 times. refactoring graded according rubric using Claude 4 Sonnet. grading results score 0 1 incorporates measures code quality well execution time. score eval mean per-sample scores multiplied 100. Grading costs something like $2.50; cost solving depends model pricing.","code":"tsk <- chores_task() tsk$eval(   solver_chat = ellmer::chat_*(model = \"some-model\") )"},{"path":"https://simonpcouch.github.io/choreseval/reference/chores.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluation results — chores","text":"","code":"chores"},{"path":"https://simonpcouch.github.io/choreseval/reference/chores.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Evaluation results — chores","text":"object class tbl_df (inherits tbl, data.frame) 8 rows 6 columns.","code":""},{"path":"https://simonpcouch.github.io/choreseval/reference/chores.html","id":"columns","dir":"Reference","previous_headings":"","what":"Columns","title":"Evaluation results — chores","text":"name: identifier experiment. provider: ellmer provider name. model: model name. score: score eval, 0 100. Scores 80 great, indicating model good fit use chores. reference, Claude 4 Sonnet scores cost: total cost run solving across 102 samples (estimated ellmer). metadata: full evaluation samples.","code":""},{"path":"https://simonpcouch.github.io/choreseval/reference/chores.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluation results — chores","text":"","code":"library(tibble)  chores #> # A tibble: 8 × 6 #>   name                          provider      model         score price metadata #>   <chr>                         <chr>         <chr>         <dbl> <chr> <list>   #> 1 claude_opus_4                 Anthropic     claude-opus-… 0.917 $7.52 <tibble> #> 2 claude_sonnet_4               Anthropic     claude-sonne… 0.939 $1.51 <tibble> #> 3 gemini_2_5_flash_non_thinking Google/Gemini gemini-2.5-f… 0.874 $0.11 <tibble> #> 4 gemini_2_5_flash_thinking     Google/Gemini gemini-2.5-f… 0.897 $0.10 <tibble> #> 5 gemini_2_5_pro                Google/Gemini gemini-2.5-p… 0.918 $0.44 <tibble> #> 6 gpt_4_1_mini                  OpenAI        gpt-4.1-mini  0.855 $0.06 <tibble> #> 7 gpt_4_1_nano                  OpenAI        gpt-4.1-nano  0.774 $0.01 <tibble> #> 8 gpt_4_1                       OpenAI        gpt-4.1       0.904 $0.29 <tibble> dplyr::glimpse(chores) #> Rows: 8 #> Columns: 6 #> $ name     <chr> \"claude_opus_4\", \"claude_sonnet_4\", \"gemini_2_5_flash_non_thi… #> $ provider <chr> \"Anthropic\", \"Anthropic\", \"Google/Gemini\", \"Google/Gemini\", \"… #> $ model    <chr> \"claude-opus-4-20250514\", \"claude-sonnet-4-20250514\", \"gemini… #> $ score    <dbl> 0.9173005, 0.9389540, 0.8741558, 0.8972495, 0.9183629, 0.8549… #> $ price    <chr> \"$7.52\", \"$1.51\", \"$0.11\", \"$0.10\", \"$0.44\", \"$0.06\", \"$0.01\"… #> $ metadata <list> [<tbl_df[102 x 12]>], [<tbl_df[102 x 12]>], [<tbl_df[102 x 1…"},{"path":"https://simonpcouch.github.io/choreseval/reference/chores_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"The chores eval — chores_dataset","title":"The chores eval — chores_dataset","text":"Pass dataset Task$new() situate inside evaluation task.","code":""},{"path":"https://simonpcouch.github.io/choreseval/reference/chores_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The chores eval — chores_dataset","text":"","code":"chores_dataset"},{"path":"https://simonpcouch.github.io/choreseval/reference/chores_dataset.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"The chores eval — chores_dataset","text":"tibble 34 rows 5 columns: id Character. Unique identifier/title code problem. input user prompt submitted chore. target Character. solution, often description notable features correct solution. source Character. URL source problem, usually link commit GitHub. NAs indicate problem written originally eval.","code":""},{"path":[]},{"path":"https://simonpcouch.github.io/choreseval/reference/chores_dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The chores eval — chores_dataset","text":"","code":"chores_dataset #> # A tibble: 34 × 5 #>    id                      input                          target source parsable #>    <chr>                   <chr>                          <chr>  <chr>  <lgl>    #>  1 ad-hoc-collapse         \"      p_names <- pset$id[is.… \"    … https… TRUE     #>  2 collapse-pluralization  \"    rlang::abort(\\n       gl… \"cli:… https… TRUE     #>  3 dbplyr-argument-inline  \"abort(\\\"DuckDB does not supp… \"cli_… https… TRUE     #>  4 dbplyr-code-inline      \"abort(\\\"Can't determine name… \"cli:… https… TRUE     #>  5 dbplyr-preserve-call    \"abort('`conflict = \\\"error\\\"… \"cli_… https… TRUE     #>  6 duckplyr-capture-output \"    abort(\\n       c(\\n     … \" ```… https… TRUE     #>  7 fn-help                 \"    rlang::abort(paste0(\\n  … \"    … https… TRUE     #>  8 fn-paste0               \"rlang::abort(paste0(\\\"Intern… \"cli:… https… TRUE     #>  9 inline-glue             \"    rlang::abort(\\n       gl… \"    … https… TRUE     #> 10 inline-variable         \"    msg <- paste0(\\\"Argument… \"cli:… https… TRUE     #> # ℹ 24 more rows  str(chores_dataset) #> tibble [34 × 5] (S3: tbl_df/tbl/data.frame) #>  $ id      : chr [1:34] \"ad-hoc-collapse\" \"collapse-pluralization\" \"dbplyr-argument-inline\" \"dbplyr-code-inline\" ... #>  $ input   : chr [1:34] \"      p_names <- pset$id[is.na(pset$object)]\\n       msg <-\\n         paste0(\\n           \\\"Some parameters do \"| __truncated__ \"    rlang::abort(\\n       glue::glue(\\n         \\\"Extra arguments will be ignored: \\\",\\n         glue::glue_col\"| __truncated__ \"abort(\\\"DuckDB does not support the `returning` argument.\\\")\" \"abort(\\\"Can't determine name for target table. Set `in_place = FALSE` to return a lazy table.\\\")\" ... #>  $ target  : chr [1:34] \"      cli::cli_abort(\\n         \\\"Some parameters do not have corresponding parameter objects and\\n          ca\"| __truncated__ \"cli::cli_abort(\\\"The extra argument{?s} {.arg {names(dots)}} will be ignored.\\\")\" \"cli_abort(\\\"DuckDB does not support the {.arg returning} argument.\\\")\" \"cli::cli_abort(\\\"Can't determine name for target table. Set {.code in_place = FALSE} to return a lazy table.\\\")\" ... #>  $ source  : chr [1:34] \"https://github.com/tidymodels/tune/commit/f6772f472039a63b0a0cf83058d1ded6c228a9a9\" \"https://github.com/tidymodels/parsnip/commit/dc9e1883ce66930ccd4497a651f0372fba5c7fac\" \"https://github.com/tidyverse/dbplyr/commit/2686125a117d40fa95bf7f997397188262ddd9b0\" \"https://github.com/tidyverse/dbplyr/commit/2686125a117d40fa95bf7f997397188262ddd9b0\" ... #>  $ parsable: logi [1:34] TRUE TRUE TRUE TRUE TRUE TRUE ..."},{"path":"https://simonpcouch.github.io/choreseval/reference/chores_scorer.html","id":null,"dir":"Reference","previous_headings":"","what":"The chores scorer — chores_scorer","title":"The chores scorer — chores_scorer","text":"Pass function Task$new() scorer evaluate model responses based chores eval criteria.","code":""},{"path":"https://simonpcouch.github.io/choreseval/reference/chores_scorer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The chores scorer — chores_scorer","text":"","code":"chores_scorer(   samples,   ...,   scorer_chat = ellmer::chat_anthropic(model = \"claude-3-7-sonnet-latest\") )"},{"path":"https://simonpcouch.github.io/choreseval/reference/chores_scorer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The chores scorer — chores_scorer","text":"samples samples solver task, likely retrieved task$get_samples(). ... Additional arguments passed scoring function. scorer_chat ellmer chat object use scoring. Defaults ellmer::chat_anthropic(model = \"claude-3-7-sonnet-latest\"); scoring model used \"official\" results.","code":""},{"path":"https://simonpcouch.github.io/choreseval/reference/chores_scorer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"The chores scorer — chores_scorer","text":"list following components: score Numeric vector scores 0 1, representing proportion criteria met. .scorer_metadata List containing prompts used scoring detailed grading results.","code":""},{"path":[]},{"path":"https://simonpcouch.github.io/choreseval/reference/chores_solver.html","id":null,"dir":"Reference","previous_headings":"","what":"The chores solver — chores_solver","title":"The chores solver — chores_solver","text":"Pass function Task$new() solver process input prompts chores dataset specified language model.","code":""},{"path":"https://simonpcouch.github.io/choreseval/reference/chores_solver.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The chores solver — chores_solver","text":"","code":"chores_solver(inputs, ..., solver_chat)"},{"path":"https://simonpcouch.github.io/choreseval/reference/chores_solver.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The chores solver — chores_solver","text":"inputs Character vector user prompts, likely chores_dataset$input. ... Additional arguments passed chat_parallel method. solver_chat ellmer chat object use solving prompts. must Chat object ellmer package.","code":""},{"path":"https://simonpcouch.github.io/choreseval/reference/chores_solver.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"The chores solver — chores_solver","text":"list following components: result Character vector model responses, one input. solver_chat List Chat objects used generate response, length inputs.","code":""},{"path":[]},{"path":"https://simonpcouch.github.io/choreseval/reference/chores_task.html","id":null,"dir":"Reference","previous_headings":"","what":"The chores evaluation task — chores_task","title":"The chores evaluation task — chores_task","text":"Creates vitals Task evaluating language models chores dataset. task combines dataset, solver, scorer single object. Run eval chores_task()$eval() solver_chat choice.","code":""},{"path":"https://simonpcouch.github.io/choreseval/reference/chores_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The chores evaluation task — chores_task","text":"","code":"chores_task(dir = \"data-raw/chores/logs\")"},{"path":"https://simonpcouch.github.io/choreseval/reference/chores_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The chores evaluation task — chores_task","text":"dir Character string specifying directory evaluation logs written.","code":""},{"path":"https://simonpcouch.github.io/choreseval/reference/chores_task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"The chores evaluation task — chores_task","text":"vitals Task object configured chores dataset, solver, scorer. object can used run evaluations task$eval().","code":""},{"path":[]}]
