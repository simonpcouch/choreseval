<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Motivation • choreseval</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Motivation">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">choreseval</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item"><a class="nav-link" href="../articles/motivation.html">Motivation</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/simonpcouch/choreseval/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Motivation</h1>
                        <h4 data-toc-skip class="author">Simon P.
Couch</h4>
            
            <h4 data-toc-skip class="date">July 2025</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/simonpcouch/choreseval/blob/main/vignettes/motivation.Rmd" class="external-link"><code>vignettes/motivation.Rmd</code></a></small>
      <div class="d-none name"><code>motivation.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/simonpcouch/choreseval" class="external-link">choreseval</a></span><span class="op">)</span></span></code></pre></div>
<p>Late last year, I published an R package called <a href="https://simonpcouch.github.io/chores/" class="external-link">chores</a>. chores provides
an extensible library of LLM assistants for R users, aimed at helping
with repetitive but hard-to-automate tasks.</p>
<p>The package works by having the user select a piece of code that
they’d like to edit and then selecting from a dropdown in a shiny app.
Each of those dropdown entries corresponds to a system prompt containing
a bunch of instructions on how to modify the code that the user
selected. I’ve found the interface super helpful for turning 45-second
tasks into 5-second ones and really appreciate the freeing up of that
mental real estate for coding tasks I find more interesting. (See <a href="https://posit.co/blog/introducing-chores/" class="external-link">this blog post</a> for
more on chores and how it works.)</p>
<p>When developing the chores package, and for LLM code-assist in
general, I use Claude 4 Sonnet, and I recommend it as the model that
people make use of with the package. Claude costs money, though—calling
a built-in chores helper 1,000 times costs <a href="https://simonpcouch.github.io/chores/#how-much-do-helpers-cost" class="external-link">something
like</a> $15 with Claude. You may or may not feel this is a good bit of
money; regardless, to use the package with Claude, users need to input
their credit card information on Anthropic’s website to test out the
package with Claude. This is a big ask just to use an R package.</p>
<p>As models that can be served locally—“for free” in the usual sense of
running code on your laptop—get better and better, I’ve wondered:
<em>Are there any LLMs that are small enough to run on a typical laptop
that are high-quality enough to make using chores worth it?</em> Up to
this point, using chores with models that can be served locally would be
more painful than just writing the code oneself.</p>
<p><em>The chores eval</em> is an LLM evaluation that I’ll be using to
try and find a free model that I can recommend chores users adopt.</p>
<div class="section level2">
<h2 id="the-eval">The eval<a class="anchor" aria-label="anchor" href="#the-eval"></a>
</h2>
<p>The chores eval measures how well a model would perform as the model
powering chores. To be a good fit for chores, a model needs to 1) write
code decently well and 2) be <em>very</em> good at
instruction-following. The eval is implemented with <a href="https://vitals.tidyverse.org/" class="external-link">vitals</a>, a new R package for
large language model evaluation that I’ve been working on. With vitals,
evals are composed of (at least) three pieces: a dataset, solver, and
scorer.</p>
<p>A <strong>dataset</strong> is, minimally a data frame of labelled
samples <code>input</code> and <code>target</code>. In our case,
<code>input</code> is composed of pieces of code “from the wild” that
need to be refactored. For example, the chores “cli” helper could
refactor this erroring code <a href="https://github.com/tidyverse/purrr/commit/06fcb5f8e9709b6796976e4b6d1ed5d67558c92a" class="external-link">from
purrr</a> to the use the cli package:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/stop.html" class="external-link">stop</a></span><span class="op">(</span><span class="st">"Cannot coerce .x to a vector"</span>, call. <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p>A successful conversion to cli would transition to
<code><a href="https://cli.r-lib.org/reference/cli_abort.html" class="external-link">cli::cli_abort()</a></code>, apply inline markup to <code>.x</code> to
note that it’s an argument (or <code>code</code>), and transition the
argument value <code>call. = FALSE</code> to its cli analogue
<code>call = NULL</code>. Here’s the <code>target</code> grading
guidance for that <code>input</code>:</p>
<pre><code>```
cli::cli_abort(
  "Can't coerce {.arg .x} to a vector."
)
```

Can also have `call = NULL`, i.e.:

```
cli::cli_abort(
  "Can't coerce {.arg .x} to a vector.", 
  call = NULL
)
```

The backticks are for example only and should not be included in the desired output.</code></pre>
<p>The next core piece is a <strong>solver</strong>, or a function that
takes <code>input</code> and, via some LLM-based tool, processes the
input to hopefully output something like the <code>target</code>. In the
case of this eval, the solver applies the <a href="https://github.com/simonpcouch/chores/blob/fbb7f1cecbcbcbd2b1b803bd16aef6a4f340d50b/inst/prompts/cli-replace.md" class="external-link">cli
chore system prompt</a> and passes it the <code>input</code> as the user
turn, returning whatever the model returns. The model powering the
solver is the important “independent variable” here.</p>
<p>The final element is the <strong>scorer</strong>. This eval uses
model grading, where the cli chore system prompt, input, target, and
solver output are all concatenated into one string. Then, all of this is
passed to <em>another</em> LLM, at which point that model is asked to
make use of all of this context to score the solver output according to
a <a href="https://github.com/simonpcouch/choreseval/blob/main/inst/prompts/rubric-cli.md#5-rubric" class="external-link">rubric</a>.
The rubric includes 14 grading criteria that can be “Yes”, “No”, or NA,
where NA indicates that that grading criterion isn’t applicable for the
given refactor. The score for that question is then calculate as
<code>n("Yes") / (n("Yes") + n("No"))</code>.</p>
<p>There are two catches here:</p>
<ul>
<li>If the solver doesn’t output valid R code for a given input, that
response is given a score of 0.</li>
<li>Solvers are given a 2-second “grace period” for their response to
finish streaming. After that, each additional second results in a point
subtracted from that numerator (with a minimum score of 0).</li>
</ul>
<p>At the end of the eval, the scores are averaged and multiplied by 100
to generate a percentage.</p>
</div>
<div class="section level2">
<h2 id="initial-benchmarks">Initial benchmarks<a class="anchor" aria-label="anchor" href="#initial-benchmarks"></a>
</h2>
<p>The usual advice for writing LLM evaluations is that, when the eval
is first released, the state-of-the-art performance should be really
low. Like, single digits low.</p>
<p>My use case for this eval is a bit different. In my case, I already
<em>have</em> a model that works really well, and I don’t need it to do
any better. <strong>Claude 4 Sonnet already performs almost
perfectly</strong>, as do many other models. If they didn’t do super
well, this would be a bug in the eval rather than a shortcoming of the
model—these problems should be easily solvable for strong LLMs.</p>
<p>The thing that I’m actually interested in is <strong>how small and/or
cheap can a model be while still performing well?</strong> Ideally, I’d
like a 8-billion parameter model, or even 4- or 2- or 1-billion
parameter model, that can return correctly refactored code within a
couple seconds. In that case, users with a laptop with 8GB or 4GB of RAM
could feasibly do so as well.</p>
<p>For consistency, I’ll be running these evals on my laptop. It’s a
4-year old—but relatively maxed-out at the time—16” Macbook Pro. In the
grand scheme of “computers that people write R code on,” this is pretty
fancy.</p>
<p>As such, there’s a sort of “weight class” dynamic here. Expensive
models that require specialized hardware to run shouldn’t be compared to
something that can be served locally on my iPhone; I recommend thinking
about these models as belonging to one of a few classes, each opening up
different modes of using and communicating about chores.</p>
<ul>
<li>Strong LLMs: These are models that are a couple to a few dollars per
million input tokens. Claude 3.7 Sonnet is $3.00, GPT-4o is $3.75 and
-4.1 is $2.00, Gemini 2.5 Pro is $1.25. These models do really well on
the eval; I’ve ran them as I developed the eval but don’t think I’ll do
so as new models come out.</li>
<li>Budget LLMs: These models are between $0.50 and $0.10 per million
inputs tokens. For example, GPT 4.1-mini is $0.40 and -nano is $0.10.
Gemini 2.0 Flash is $0.35. In the example of GPT 4.1-nano, this is
<em>thirty times</em> cheaper than Claude 3.7 Sonnet. That’s 2,000
refactors per dollar. If such a model is a capable helper, I could
confidently tell a user “Put $1.00 on this API key and you will never
have to reload it.”</li>
<li>Local LLMs: Anything I can run on my Macbook. My system has 32GB of
RAM, but a 32-billion parameter model would probably run so slowly that
the speed penalty would overtake strong performance. My guess would be
that, at the moment, there a couple of models available today that are
&gt;50% on this eval.</li>
<li>SmoLLMs: Maximum of 4-billion parameters. This is ultimately what
I’m <em>really</em> interested in.</li>
</ul>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Simon Couch, Posit Software, PBC.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
