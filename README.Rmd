---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
#| label: setup
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# The chores eval

<!-- badges: start -->

[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental) [![CRAN status](https://www.r-pkg.org/badges/version/choreseval)](https://CRAN.R-project.org/package=choreseval)

<!-- badges: end -->

The [chores](https://posit.co/blog/introducing-chores/) package connects an extensible library of LLM assistants to your IDE aimed at helping you with tedious but hard-to-automate tasks. For the most capable LLMs today, like Anthropic's Claude 4 Sonnet or OpenAI's GPT 4.1, carrying out the sorts of tasks that chores supports is easy peasy. However, those models cost money (and require entrusting your IP with a for-profit company), and it's a bit of a hoot to ask R users to put their credit card information down to use an R package.

This has made me wonder: **Is it possible to use chores with a locally running LLM?** The chores eval evaluates how well a large language model will perform as the model powering chores with the goal of helping me identify a model that chores users can run themselves on a modest laptop. 

The chores eval is implemented with [vitals](https://vitals.tidyverse.org/), an LLM eval framework for R.

## Installation

choreseval is implemented as an R package for ease of installation:

``` r
pak::pak("simonpcouch/choreseval")
```

Load it with:

```{r}
# label: load
library(choreseval)
```

## Example

As a reader of the eval, you're mostly likely interested in `chores`, a dataset of compiled evaluation results:

```{r}
#| label: chores
library(tibble)

chores
```

```{r}
#| label: chores-plot
#| warning: false
#| message: false
library(tidyverse)
library(ggrepel)

chores |>
  mutate(price = as.numeric(gsub("$", "", price, fixed = TRUE))) |>
  ggplot(aes(x = price, y = score, color = provider, label = model)) +
  geom_point() +
  geom_label_repel() +
  scale_x_log10(labels = scales::dollar_format()) +
  theme_minimal()
```

The `chores_task()` function defines a task with the package's built-in dataset, solver, and scorer:

```{r}
#| label: chores-task
tsk <- chores_task()

tsk
```

Run `$eval()` with the `solver_chat` of your choice to measure how well that model does on the eval:

```{r}
#| label: chores-eval
#| eval: false
tsk$eval(
  solver_chat = ellmer::chat_anthropic(model = "claude-3-7-sonnet-latest")
)
```
